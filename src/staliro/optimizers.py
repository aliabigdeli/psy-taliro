from __future__ import annotations

from abc import ABC, abstractmethod
from collections.abc import Iterable, Sequence
from enum import IntEnum
from itertools import takewhile
from os import cpu_count
from statistics import mean
from typing import Any, Generic, Literal, Protocol, TypeVar, overload

from attrs import frozen
from numpy import float_
from numpy.random import Generator, default_rng
from numpy.typing import NDArray
from scipy import optimize
from typing_extensions import TypeAlias

from .cost_func import SampleLike
from .options import Interval

R = TypeVar("R", covariant=True)


class ObjFunc(Protocol[R]):
    """Representation of a function that can be optimized by an optimizer.

    An objective function evaluates samples generated by the optimizer and returns a scalar cost
    value for each sample. The objective function is capable of evaluating one or more samples at
    once.
    """

    def eval_sample(self, sample: SampleLike) -> R:
        """Evaluate a single sample.

        Arguments:
            sample: The sample to evaluate

        Returns:
            The cost of the sample
        """
        ...

    def eval_samples(self, samples: Sequence[SampleLike]) -> Sequence[R]:
        """Evaluate several samples sequentially.

        Arguments:
            samples: A sequence of samples to evaluate

        Returns:
            The cost of each sample in the same order they were given
        """
        return [self.eval_sample(s) for s in samples]


C = TypeVar("C", contravariant=True)


class Optimizer(Generic[C, R], ABC):
    """An optimizer selects samples to be evaluated by the cost function."""

    @frozen(slots=True)
    class Params:
        seed: int
        budget: int
        input_bounds: Sequence[Interval]

    @abstractmethod
    def optimize(self, func: ObjFunc[C], params: Params) -> R:
        """Evaluate samples and use the result to select more samples until the budget is reached.

        The optimize method is responsible for generating samples from the
        """
        ...


class Behavior(IntEnum):
    """Behavior when falsifying case for system is encountered.

    Attributes:
        FALSIFICATION: Stop searching when the first falsifying case is encountered
        MINIMIZATION: Continue searching after encountering a falsifying case until iteration
                      budget is exhausted
    """

    FALSIFICATION = 0
    MINIMIZATION = 1


Samples: TypeAlias = Sequence[SampleLike]


def _sample_uniform(bounds: Iterable[Interval], rng: Generator) -> list[float]:
    return [rng.uniform(bound.lower, bound.upper) for bound in bounds]


def _minimize(samples: Samples, func: ObjFunc[float]) -> Iterable[float]:
    return func.eval_samples(samples)


def _falsify(samples: Samples, func: ObjFunc[float]) -> Iterable[float]:
    return takewhile(lambda c: c >= 0, map(func.eval_sample, samples))


@frozen(slots=True)
class UniformRandomResult:
    """Data class that represents the result of a uniform random optimization.

    Attributes:
        average_cost: The average cost of all the samples selected.
    """

    average_cost: float


class UniformRandom(Optimizer[float, UniformRandomResult]):
    """Optimizer that implements the uniform random optimization technique.

    This optimizer picks samples randomly from the search space until the budget is exhausted.

    Args:
        parallelization: Value that indicates how many processes to use when evaluating each
                            sample using the cost function. Acceptable values are a number,
                            "cores", or None

    Attributes:
        processes: The number of processes to use when evaluating the samples.
    """

    def __init__(
        self,
        parallelization: Literal["cores"] | int | None = None,
        behavior: Behavior = Behavior.FALSIFICATION,
    ):
        if parallelization == "cores":
            self.processes: int | None = cpu_count()
        elif isinstance(parallelization, int):
            self.processes = parallelization
        else:
            self.processes = None

        self.behavior = behavior

    def optimize(self, func: ObjFunc[float], params: Optimizer.Params) -> UniformRandomResult:
        rng = default_rng(params.seed)
        samples = [_sample_uniform(params.input_bounds, rng) for _ in range(params.budget)]

        if self.behavior is Behavior.MINIMIZATION:
            costs = _minimize(samples, func)
        else:
            costs = _falsify(samples, func)

        average_cost = mean(costs)

        return UniformRandomResult(average_cost)


@frozen(slots=True)
class DualAnnealingResult:
    """Data class representing the result of a dual annealing optimization.

    Attributes:
        jacobian_value: The value of the cost function jacobian at the minimum cost discovered
        jacobian_evals: Number of times the jacobian of the cost function was evaluated
        hessian_value: The value of the cost function hessian as the minimum cost discovered
        hessian_evals: Number of times the hessian of the cost function was evaluated
    """

    jacobian_value: NDArray[float_] | None
    jacobian_evals: int
    hessian_value: NDArray[float_] | None
    hessian_evals: int


class DualAnnealing(Optimizer[float, DualAnnealingResult]):
    """Optimizer that implements the simulated annealing optimization technique.

    The simulated annealing implementation is provided by the SciPy library dual_annealing function
    with the no_local_search parameter set to True.
    """

    def __init__(self, behavior: Behavior = Behavior.FALSIFICATION):
        self.behavior = behavior

    def optimize(self, func: ObjFunc[float], params: Optimizer.Params) -> DualAnnealingResult:
        def listener(sample: Any, cost: float, ctx: Literal[-1, 0, 1]) -> bool:
            if cost < 0 and self.behavior is Behavior.FALSIFICATION:
                return True

            return False

        result = optimize.dual_annealing(
            func=lambda x: func.eval_sample(x),
            bounds=[bound.astuple() for bound in params.input_bounds],
            seed=params.seed,
            maxfun=params.budget,
            no_local_search=True,  # Disable local search, use only traditional generalized SA
            callback=listener,
        )

        try:
            jac: NDArray[float_] | None = result.jac
            njev = result.njev
        except AttributeError:
            jac = None
            njev = 0

        try:
            hess: NDArray[float_] | None = result.hess
            nhev = result.nhev
        except AttributeError:
            hess = None
            nhev = 0

        return DualAnnealingResult(jac, njev, hess, nhev)


class UserFunc(Protocol[C, R]):
    def __call__(self, __func: ObjFunc[C], __params: Optimizer.Params) -> R:
        ...


class UserOptimizer(Optimizer[C, R]):
    def __init__(self, func: UserFunc[C, R]):
        self.func = func

    def optimize(self, func: ObjFunc[C], params: Optimizer.Params) -> R:
        return self.func(func, params)


class Decorator(Protocol):
    def __call__(self, __f: UserFunc[C, R]) -> UserOptimizer[C, R]:
        ...


T = TypeVar("T", covariant=True)
U = TypeVar("U", covariant=True)


@overload
def optimizer(func: UserFunc[C, R]) -> UserOptimizer[C, R]:
    ...


@overload
def optimizer(func: None = ...) -> Decorator:
    ...


def optimizer(func: UserFunc[C, R] | None = None) -> UserOptimizer[C, R] | Decorator:
    def _decorator(func: UserFunc[T, U]) -> UserOptimizer[T, U]:
        return UserOptimizer(func)

    return _decorator(func) if func else _decorator
