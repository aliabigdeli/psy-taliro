"""
Generate samples for the cost function to evaluate.

An optimizer is responsible for generating the samples that are evaluated by the cost function.
From the evaluation of each sample the ``Optimizer`` receives a cost value, which represents some
quality of the sample that should either be minimized or maximized. Samples should be generated
until some stopping condition is reached, either from the budget being exhausted or the cost of a
sample meets some criterion. You can create an optimizer by inheriting from the `Optimizer` class,
which has one required method called `optimize()` that should accept as input an `ObjFunc` function
to use for sample evaluation, and a `Optimizer.Params` value containing the optimization parameters
like the budget and initial seed. You can also construct an optimizer by decorating a function with
the `optimizer()` decorator. The decorated function should accept the sample inputs as the
``optimize()`` method.

::

    from staliro import optimizers

    class Optimizer(optimizers.Optimizer[float, None]):
        def optimize(self, func: optimizers.ObjFunc[float], params: optimizers.Optimizer.Params) -> None:
            ...


    @optimizers.optimizer()
    def optimizer(func: optimizers.ObjFunc[float], params: optimizers.Optimizer.Params) -> None:
        ...


Uniform Random
--------------

This module provides the `UniformRandom` optimizer, which uniformly samples the input space.
You can configure the optimizer to exit early if a cost threshold is reached by providing the
``min_cost`` argument to the constructor. This optimizer has no requirements about the type
of the cost value so long as, if a ``min_cost`` is provided, the value supports comparison.

::

    from staliro.optimizers import UniformRandom

    opt = UniformRandom()
    opt = UniformRandom(min_cost=0.0)


Simulated Annealing
-------------------

This module provides the `DualAnnealing` optimizer, which utilizes the general simulated annealing
implementation ``dual_annealing()`` from the SciPy library. When constructing the optimizer, you
can provide a cost threshold using the ``min_cost`` argument to the constructor. The cost values
returned to the optimizer must be `float`.

::

    from staliro.optimizers import DualAnnealing

    opt = DualAnnealing()
    opt = DualAnnealing(min_cost=0.0)
"""

from __future__ import annotations

from abc import ABC, abstractmethod
from collections.abc import Iterable, Sequence
from typing import Generic, Literal, Protocol, TypeVar, overload
import os
from openai import OpenAI
import json
import re

from attrs import frozen
from numpy import float_
from numpy.random import Generator, default_rng
from numpy.typing import NDArray
from scipy import optimize
from typing_extensions import TypeAlias

from .cost_func import SampleLike
from .options import Interval

R = TypeVar("R", covariant=True)


class ObjFunc(Protocol[R]):
    """Representation of a function that can be optimized by an optimizer.

    An objective function evaluates samples generated by the optimizer and returns a scalar cost
    value for each sample. The objective function is capable of evaluating one or more samples at
    once.
    """

    def eval_sample(self, sample: SampleLike) -> R:
        """Evaluate a single sample.

        :param sample: The sample to evaluate
        :returns: The cost of the sample
        """

        ...

    def eval_samples(self, samples: Iterable[SampleLike]) -> Sequence[R]:
        """Evaluate a batch of samples.

        :param samples: A sequence of samples to evaluate
        :returns: The cost of each sample in the same order they were given
        """

        return [self.eval_sample(s) for s in samples]


C = TypeVar("C", contravariant=True)


class Optimizer(Generic[C, R], ABC):
    """An optimizer selects samples to be evaluated by the cost function.

    This class is parameterized by two type variables, ``C`` and ``R``. ``C`` is the type of the
    cost value that the optimizer expects to receive from the cost function, and ``R`` represents
    the type that the optimizer will return at the end of an optimization attempt.
    """

    @frozen(slots=True)
    class Params:
        """The parameters for an optimization attempt.

        :attribute seed: The value to use to seed a random number generator for reproducibility
        :attribute budget: The maximum number of samples to evaluate
        :attribute input_bounds: The search range to for each input variable
        """

        seed: int
        budget: int
        input_bounds: Sequence[Interval]

    @abstractmethod
    def optimize(self, func: ObjFunc[C], params: Params) -> R:
        """Evaluate samples and use the result to select more samples until the budget is reached.

        The optimize method is responsible for generating samples that will be evaluated by the cost
        function into cost values that can be used to inform the selection of subsequent samples. In
        order to receive cost values, implementations must call either the ``eval_sample`` or
        ``eval_samples`` methods on the ``func`` value.

        :param func: The cost function to use for evaluating samples
        :param params: The optimization parameters
        :returns: The cost value
        """
        ...


Samples: TypeAlias = Iterable[SampleLike]


class Comparable(Protocol):
    @abstractmethod
    def __lt__(self: CT, other: CT) -> bool: ...


CT = TypeVar("CT", bound=Comparable)


def _sample_uniform(bounds: Iterable[Interval], rng: Generator) -> list[float]:
    return [rng.uniform(bound[0], bound[1]) for bound in bounds]


def _minimize(samples: Samples, func: ObjFunc[object]) -> None:
    func.eval_samples(samples)


def _falsify(samples: Samples, func: ObjFunc[CT], min_cost: CT | None, max_cost: CT | None) -> None:
    for sample in samples:
        cost = func.eval_sample(sample)

        if min_cost and cost < min_cost:
            break

        if max_cost and cost > max_cost:
            break


class UniformRandom(Optimizer[CT, None]):
    """Optimizer that samples the input space uniformly.

    This optimizer will exhaust the sample budget unless the ``min_cost`` argument is provided. If a
    minimum cost is indicated then the optimizer will terminate early if a cost is found below that
    value.

    :param min_cost: The minimum cost that will cause the optimize to terminate
    """

    def __init__(self, min_cost: CT | None = None, max_cost: CT | None = None):
        self.min_cost = min_cost
        self.max_cost = max_cost

    def optimize(self, func: ObjFunc[CT], params: Optimizer.Params) -> None:
        rng = default_rng(params.seed)
        samples = [_sample_uniform(params.input_bounds, rng) for _ in range(params.budget)]

        if self.min_cost or self.max_cost:
            return _falsify(samples, func, self.min_cost, self.max_cost)

        return _minimize(samples, func)


@frozen(slots=True)
class DualAnnealingResult:
    """Data class containing additional data from a dual annealing optimization.

    :attribute jacobian_value: The value of the cost function jacobian at the minimum cost discovered
    :attribute jacobian_evals: Number of times the jacobian of the cost function was evaluated
    :attribute hessian_value: The value of the cost function hessian as the minimum cost discovered
    :attribute hessian_evals: Number of times the hessian of the cost function was evaluated
    """

    jacobian_value: NDArray[float_] | None
    jacobian_evals: int
    hessian_value: NDArray[float_] | None
    hessian_evals: int


class DualAnnealing(Optimizer[float, DualAnnealingResult]):
    """Optimizer implementing generalized simulated annealing.

    The simulated annealing implementation is provided by the SciPy library dual_annealing function
    with the no_local_search parameter set to True. This optimizer will exhaust the sample budget
    unless the ``min_cost`` argument is provided in the constructor. If ``min_cost`` is provided, then
    the optimizer will exit if a cost value is found that is less than the value.

    :param min_cost: The minimum cost to use as a termination condition
    """

    def __init__(self, min_cost: float | None = None):
        self.min_cost = min_cost

    def optimize(self, func: ObjFunc[float], params: Optimizer.Params) -> DualAnnealingResult:
        def listener(sample: object, cost: float, ctx: Literal[-1, 0, 1]) -> bool:
            if self.min_cost and cost < self.min_cost:
                return True

            return False

        result = optimize.dual_annealing(
            func=lambda x: func.eval_sample(x),
            bounds=list(params.input_bounds),
            seed=params.seed,
            maxfun=params.budget,
            no_local_search=True,  # Disable local search, use only traditional generalized SA
            callback=listener,
        )

        try:
            jac: NDArray[float_] | None = result.jac
            njev = result.njev
        except AttributeError:
            jac = None
            njev = 0

        try:
            hess: NDArray[float_] | None = result.hess
            nhev = result.nhev
        except AttributeError:
            hess = None
            nhev = 0

        return DualAnnealingResult(jac, njev, hess, nhev)


class UserFunc(Protocol[C, R]):
    def __call__(self, __func: ObjFunc[C], __params: Optimizer.Params) -> R: ...


class UserOptimizer(Optimizer[C, R]):
    def __init__(self, func: UserFunc[C, R]):
        self.func = func

    def optimize(self, func: ObjFunc[C], params: Optimizer.Params) -> R:
        return self.func(func, params)


class Decorator(Protocol):
    def __call__(self, __f: UserFunc[C, R]) -> UserOptimizer[C, R]: ...


T = TypeVar("T", covariant=True)
U = TypeVar("U", covariant=True)


@overload
def optimizer(func: UserFunc[C, R]) -> UserOptimizer[C, R]: ...


@overload
def optimizer(func: None = ...) -> Decorator: ...


def optimizer(func: UserFunc[C, R] | None = None) -> UserOptimizer[C, R] | Decorator:
    """Create an `Optimizer` from a function.

    The provided function must accept a `ObjFunc` and a `Optimizer.Params` as arguments. If no
    function is provided, a decorator will be returned that can be called with the function.

    :param func: The function to use as an optimizer
    :returns: An ``Optimizer`` or a decorator to construct an ``Optimizer``
    """

    def _decorator(func: UserFunc[T, U]) -> UserOptimizer[T, U]:
        return UserOptimizer(func)

    return _decorator(func) if func else _decorator


@frozen(slots=True)
class LLMOptimizerResult:
    """Data class containing additional data from LLM optimization.

    :attribute best_sample: The best sample found during optimization
    :attribute best_cost: The cost of the best sample
    :attribute history: List of (sample, cost) pairs from the optimization history
    :attribute num_evals: Number of function evaluations performed
    """

    best_sample: list[float]
    best_cost: float
    history: list[tuple[list[float], float]]
    num_evals: int


class LLMOptimizer(Optimizer[float, LLMOptimizerResult]):
    """Optimizer implementing LLM-based optimization.

    This optimizer uses a Large Language Model to generate and optimize samples based on previous
    evaluations. The LLM is prompted with the optimization history and asked to generate new samples
    that are likely to improve upon previous results.

    :param model_name: Name of the LLM model to use (e.g. "gpt-3.5-turbo")
    :param min_cost: The minimum cost to use as a termination condition
    :param temperature: Temperature parameter for LLM sampling (0.0 to 1.0)
    :param max_history: Maximum number of previous samples to include in LLM prompt
    """

    def __init__(
        self,
        model_name: str = "gpt-4.1-nano",
        min_cost: float | None = None,
        temperature: float = 0.7,
        max_history: int = 10,
    ):
        self.model_name = model_name
        self.min_cost = min_cost
        self.temperature = temperature
        self.max_history = max_history
        
        # Read API key from file and set environment variable
        try:
            with open("../api_key.txt", "r") as f:
                api_key = f.read().strip()
            os.environ["OPENAI_API_KEY"] = api_key
            self.client = OpenAI()
        except FileNotFoundError:
            raise FileNotFoundError("api_key.txt file not found. Please create this file with your OpenAI API key.")
        except Exception as e:
            raise RuntimeError(f"Failed to initialize OpenAI client: {e}")

    def _create_prompt(self, bounds: Sequence[Interval], history: list[tuple[list[float], float]]) -> str:
        """Create a prompt for the LLM based on optimization history."""
        prompt = f"""You are an optimization assistant. Your task is to generate a new sample point that will minimize the objective function.

The input space has {len(bounds)} dimensions with the following bounds:
{chr(10).join(f"Dimension {i}: [{bound[0]}, {bound[1]}]" for i, bound in enumerate(bounds))}

Here are the previous {min(len(history), self.max_history)} samples and their costs:
{chr(10).join(f"Sample {i}: {sample} -> Cost: {cost}" for i, (sample, cost) in enumerate(history[-self.max_history:]))}

Based on this history, generate a new sample point that is likely to have a lower cost.
Return only the sample point as a comma-separated list of numbers within the bounds.
"""
        return prompt

    def _generate_sample(self, prompt: str) -> list[float]:
        """Generate a new sample using the LLM.
        
        :param prompt: The prompt to send to the LLM
        :returns: A list of floats representing the new sample point
        :raises ValueError: If the LLM response cannot be parsed into a valid sample
        """
        try:
            # Call OpenAI API using the new client interface
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are an optimization assistant that generates sample points as comma-separated numbers."},
                    {"role": "user", "content": prompt}
                ],
                temperature=self.temperature,
                max_tokens=100
            )
            
            # Extract the response text
            response_text = response.choices[0].message.content.strip()
            
            # Try to parse the response as a list of numbers
            # First, try to find numbers in the text using regex
            numbers = re.findall(r'-?\d*\.?\d+', response_text)
            
            if not numbers:
                raise ValueError("No numbers found in LLM response")
                
            # Convert to floats
            sample = [float(num) for num in numbers]
            
            return sample
            
        except Exception as e:
            # If anything goes wrong, fall back to random sampling
            print(f"Error generating sample: {e}")
            import random
            return [random.uniform(0, 1) for _ in range(10)]

    def optimize(self, func: ObjFunc[float], params: Optimizer.Params) -> LLMOptimizerResult:
        history: list[tuple[list[float], float]] = []
        best_sample: list[float] = []
        best_cost = float('inf')
        num_evals = 0

        # Generate initial random sample
        rng = default_rng(params.seed)
        current_sample = _sample_uniform(params.input_bounds, rng)
        current_cost = func.eval_sample(current_sample)
        history.append((current_sample, current_cost))
        best_sample = current_sample
        best_cost = current_cost
        num_evals += 1

        while num_evals < params.budget:
            # Create prompt for LLM
            prompt = self._create_prompt(params.input_bounds, history)
            
            # Generate new sample using LLM
            new_sample = self._generate_sample(prompt)
            
            # Ensure sample is within bounds
            new_sample = [
                max(min(x, bound[1]), bound[0])
                for x, bound in zip(new_sample, params.input_bounds)
            ]
            
            # Evaluate new sample
            new_cost = func.eval_sample(new_sample)
            history.append((new_sample, new_cost))
            num_evals += 1

            # Update best sample if needed
            if new_cost < best_cost:
                best_sample = new_sample
                best_cost = new_cost

            # Check termination condition
            if self.min_cost and best_cost <= self.min_cost:
                break

        return LLMOptimizerResult(
            best_sample=best_sample,
            best_cost=best_cost,
            history=history,
            num_evals=num_evals
        )
