from __future__ import annotations

from collections.abc import Iterable, Sequence
from enum import IntEnum
from itertools import takewhile
from os import cpu_count
from statistics import mean
from typing import Literal, Protocol, TypeVar

from attrs import frozen
from numpy.random import Generator, default_rng
from numpy.typing import NDArray
from numpy import float_
from scipy import optimize
from typing_extensions import TypeAlias

from .options import Interval
from .tests import TestOptions


class Sample:
    pass


ResultT = TypeVar("ResultT", covariant=True)


class ObjFunc(Protocol[ResultT]):
    """Representation of a function that can be optimized by an optimizer.

    An objective function evaluates samples generated by the optimizer and returns a scalar cost
    value for each sample. The objective function is capable of evaluating one or more samples at
    once.
    """

    def eval_sample(self, sample: Sample) -> ResultT:
        """Evaluate a single sample.

        Arguments:
            sample: The sample to evaluate

        Returns:
            The cost of the sample
        """
        ...

    def eval_samples(self, samples: Sequence[Sample]) -> Sequence[ResultT]:
        """Evaluate several samples sequentially.

        Arguments:
            samples: A sequence of samples to evaluate

        Returns:
            The cost of each sample in the same order they were given
        """
        ...

    def eval_samples_parallel(self, samples: Sequence[Sample], nprocs: int) -> Sequence[ResultT]:
        """Evaluate several samples parallelizing according to the number of processes.

        Arguments:
            samples: A sequence of samples to evaluate
            nprocs: The number of processes to use to evaluate the samples

        Returns:
            The cost of each sample in the sample order they were given
        """
        ...


CostT = TypeVar("CostT", contravariant=True)


class Optimizer(Protocol[CostT, ResultT]):
    """An optimizer selects samples to be evaluated by the cost function."""

    def __call__(self, func: ObjFunc[CostT], options: TestOptions) -> ResultT:
        """Evaluate samples and use the result to select more samples until the budget is reached.

        The optimize method is responsible for generating samples from the
        """
        ...


class Behavior(IntEnum):
    """Behavior when falsifying case for system is encountered.

    Attributes:
        FALSIFICATION: Stop searching when the first falsifying case is encountered
        MINIMIZATION: Continue searching after encountering a falsifying case until iteration
                      budget is exhausted
    """

    FALSIFICATION = 0
    MINIMIZATION = 1


Samples: TypeAlias = Sequence[Sample]


def _sample_uniform(bounds: list[Interval], rng: Generator) -> Sample:
    return Sample([rng.uniform(bound.lower, bound.upper) for bound in bounds])


def _minimize(samples: Samples, func: ObjFunc[float], nprocs: int | None) -> Iterable[float]:
    if nprocs is None:
        return func.eval_samples(samples)
    else:
        return func.eval_samples_parallel(samples, nprocs)


def _falsify(samples: Samples, func: ObjFunc[float]) -> Iterable[float]:
    return takewhile(lambda c: c >= 0, map(func.eval_sample, samples))


@frozen(slots=True)
class UniformRandomResult:
    """Data class that represents the result of a uniform random optimization.

    Attributes:
        average_cost: The average cost of all the samples selected.
    """

    average_cost: float


class UniformRandom(Optimizer[float, UniformRandomResult]):
    """Optimizer that implements the uniform random optimization technique.

    This optimizer picks samples randomly from the search space until the budget is exhausted.

    Args:
        parallelization: Value that indicates how many processes to use when evaluating each
                            sample using the cost function. Acceptable values are a number,
                            "cores", or None

    Attributes:
        processes: The number of processes to use when evaluating the samples.
    """

    def __init__(
        self,
        parallelization: Literal["cores"] | int | None = None,
        behavior: Behavior = Behavior.FALSIFICATION,
    ):
        if parallelization == "cores":
            self.processes: int | None = cpu_count()
        elif isinstance(parallelization, int):
            self.processes = parallelization
        else:
            self.processes = None

        self.behavior = behavior

    def optimize(self, func: ObjFunc[float], options: TestOptions) -> UniformRandomResult:
        rng = default_rng(options.seed)
        samples = [_sample_uniform(options.input_bounds, rng) for _ in range(options.budget)]

        if self.behavior is Behavior.MINIMIZATION:
            costs = _minimize(samples, func, self.processes)
        else:
            costs = _falsify(samples, func)

        average_cost = mean(costs)

        return UniformRandomResult(average_cost)


@frozen(slots=True)
class DualAnnealingResult:
    """Data class representing the result of a dual annealing optimization.

    Attributes:
        jacobian_value: The value of the cost function jacobian at the minimum cost discovered
        jacobian_evals: Number of times the jacobian of the cost function was evaluated
        hessian_value: The value of the cost function hessian as the minimum cost discovered
        hessian_evals: Number of times the hessian of the cost function was evaluated
    """

    jacobian_value: NDArray[float_] | None
    jacobian_evals: int
    hessian_value: NDArray[float_] | None
    hessian_evals: int


class DualAnnealing(Optimizer[float, DualAnnealingResult]):
    """Optimizer that implements the simulated annealing optimization technique.

    The simulated annealing implementation is provided by the SciPy library dual_annealing function
    with the no_local_search parameter set to True.
    """

    def __init__(self, behavior: Behavior = Behavior.FALSIFICATION):
        self.behavior = behavior

    def __call__(self, func: ObjFunc[float], options: TestOptions) -> DualAnnealingResult:
        def listener(sample: Any, cost: float, ctx: Literal[-1, 0, 1]) -> bool:
            if cost < 0 and self.behavior is Behavior.FALSIFICATION:
                return True

            return False

        result = optimize.dual_annealing(
            func=lambda x: func.eval_sample(Sample(x)),
            bounds=[bound.astuple() for bound in options.input_bounds],
            seed=options.seed,
            maxfun=options.budget,
            no_local_search=True,  # Disable local search, use only traditional generalized SA
            callback=listener,
        )

        try:
            jac: NDArray[float_] | None = result.jac
            njev = result.njev
        except AttributeError:
            jac = None
            njev = 0

        try:
            hess: NDArray[float_] | None = result.hess
            nhev = result.nhev
        except AttributeError:
            hess = None
            nhev = 0

        return DualAnnealingResult(jac, njev, hess, nhev)
